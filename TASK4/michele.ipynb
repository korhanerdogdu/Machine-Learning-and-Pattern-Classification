{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from skimage import filters\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import EfficientNetV2B3, EfficientNetV2M\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Lambda,\n",
    "    MaxPooling2D,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import yaml\n",
    "from tensorflow.keras.models import load_model\n",
    "import csv\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import median_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningModelBuilder:\n",
    "    def __init__(self, model_params):\n",
    "        self.input_shape = model_params[\"input_shape\"]\n",
    "        self.learning_rate = model_params.get(\"learning_rate\", 0.001)\n",
    "        self.l1 = model_params.get(\"l1\", 0.0)\n",
    "        self.base_model_name = model_params.get(\"base_model_name\", \"EfficientNetV2B3\")\n",
    "        self.weight_initialization = model_params.get(\"weight_initialization\", \"he_normal\")\n",
    "        self.last_layers_to_train = model_params.get(\"last_layers_to_train\", 0)\n",
    "        self.dropout = model_params.get(\"dropout\", 0.0)\n",
    "        self.num_classes = model_params.get(\"num_classes\", 1)  # Number of output classes\n",
    "        self.model = None\n",
    "        self.base_model = None\n",
    "\n",
    "    def build_base_model(self):\n",
    "        # Input layer for the grayscale image\n",
    "        input_layer = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        # Check if the image is grayscale (channel dimension is 1)\n",
    "        if self.input_shape[-1] == 1:\n",
    "            # Lambda layer to repeat the grayscale channel three times\n",
    "            x = Lambda(lambda x: tf.repeat(x, 3, axis=-1))(input_layer)\n",
    "        else:\n",
    "            x = input_layer\n",
    "\n",
    "        # Base model initialization\n",
    "        if self.base_model_name == \"EfficientNetV2B3\":\n",
    "            self.base_model = EfficientNetV2B3(weights=\"imagenet\", include_top=False, input_tensor=x)\n",
    "        elif self.base_model_name == \"EfficientNetV2L\":\n",
    "            self.base_model = EfficientNetV2M(weights=\"imagenet\", include_top=False, input_tensor=x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid base model name\")\n",
    "\n",
    "        # Freeze all layers of base model for transfer learning\n",
    "        for layer in self.base_model.layers[: -self.last_layers_to_train]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        return Model(inputs=input_layer, outputs=self.base_model.output)\n",
    "\n",
    "    def build(self):\n",
    "        base_model = self.build_base_model()\n",
    "        x = Flatten()(base_model.output)  # Flatten the output to connect with Dense layer\n",
    "\n",
    "        # Classifier with L1 regularization\n",
    "        if self.dropout > 0.0:\n",
    "            x = tf.keras.layers.Dropout(self.dropout)(x)\n",
    "        output = Dense(\n",
    "            21,  # Change to number of classes for multi-class classification\n",
    "            activation=\"softmax\",  # Use softmax for multi-class classification\n",
    "            kernel_regularizer=regularizers.l1(self.l1),\n",
    "            kernel_initializer=self.weight_initialization,\n",
    "        )(x)\n",
    "\n",
    "        self.model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    def compile(self):\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "            loss=\"categorical_crossentropy\",  # Use categorical_crossentropy for multi-class classification\n",
    "            metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC(name='auc')],\n",
    "        )\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TransferLearningModelBuilder instance with parameters from YAML file\n",
    "model_builder = TransferLearningModelBuilder(config['model_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model_builder.build()\n",
    "model = model_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for training and validation data\n",
    "train_dir = 'train_directory'\n",
    "val_dir = 'val_directory'\n",
    "\n",
    "# Create ImageDataGenerator for training and validation\n",
    "train_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create generators for training and validation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'  # Use 'categorical' for multi-class classification\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory=val_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'  # Use 'categorical' for multi-class classification\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to save the model at each epoch\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Implement early stopping to avoid overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, validation_data=val_generator, epochs=10, callbacks=[checkpoint_callback, early_stopping])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
